{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning binary classifier\n",
    "## used to detect sentence boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data preparation and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696496 1696496 <type 'list'>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import nltk\n",
    "from __future__ import print_function\n",
    "from six.moves import xrange\n",
    "import six.moves.cPickle as pickle\n",
    "import random\n",
    "t0 = time.time()\n",
    "\n",
    "en = \"/home/work/ssCat/sss.en\"\n",
    "tokens = []  #store all the tokes\n",
    "boundaries = set()   #store boundary index\n",
    "boundaries_token = set()  #store tokens which is a sentence boundary\n",
    "known = 0.9 #only register 90% of words as known\n",
    "sents = []\n",
    "with open(en) as inputen:\n",
    "    offset = 0\n",
    "    for sent in inputen:\n",
    "        #nltk tokenization\n",
    "        sent = nltk.word_tokenize(sent.decode(\"utf-8\")) \n",
    "        if len(sent) >=1:\n",
    "            sents.append(sent)\n",
    "            boundaries_token.add(sent[-1])\n",
    "            tokens.extend(sent)\n",
    "            offset += len(sent)\n",
    "            boundaries.add(offset-1)\n",
    "#frequent words will assaign small number\n",
    "vocabulary_size = int(len(set(tokens))*known)\n",
    "count = []\n",
    "count.extend(collections.Counter(tokens).most_common(vocabulary_size - 1))\n",
    "words = [a for a,b in count]\n",
    "word2id = {w:i for i,w in enumerate(words)} \n",
    "\n",
    "#save dic\n",
    "def save_word():\n",
    "    pickle.dump(word2id,open(\"word2id.dict\",\"w\"))\n",
    "fdic = {}\n",
    "\n",
    "def get_feature(tokens,i):\n",
    "    \"\"\"The following features are used for English:\n",
    "    boundary token\n",
    "    previous word of boundary token\n",
    "    next word of boundary token\n",
    "    boolean if previous word is digit\n",
    "    boolean if the first letter of the boundary token is capital\n",
    "    boolean if the first letter of the next word is capital\n",
    "    boolean if the first letter of previous word is capital\n",
    "    boolean if the first letter of the previous 2 word is capital\"\"\"  \n",
    "    return ['punct=%s'%tokens[i] if tokens[i] in words else \"UNK\",\\\n",
    "            'prepunct=%s'%tokens[i-1] if tokens[i] in words else \"UNK\",\\\n",
    "            'nextpunct=%s'%tokens[i+1] if tokens[i] in words else \"UNK\",\\\n",
    "            'prepunctdigit=%s'%tokens[i-1].isdigit(),\\\n",
    "            'punctCapi=%s'%tokens[i][0].isupper(),\\\n",
    "            'next1letterCapi=%s'%tokens[i+1][0].isupper(),\\\n",
    "            'pre1LetterCap=%s'%tokens[i-1][0].isupper(),\\\n",
    "            'pre2LetterCap=%s'%tokens[i-2][0].isupper()]\n",
    "\n",
    "#make a feature dic     \n",
    "\n",
    "def get_feature_from_token():\n",
    "    feature_sets = []\n",
    "    for i in xrange(2, len(tokens)-2):\n",
    "        feature_sets.extend(get_feature(tokens,i))\n",
    "    return feature_sets\n",
    "feature_sets = get_feature_from_token()        \n",
    "        \n",
    "print (len(feature_sets),(len(tokens)-4)*8,type(feature_sets))\n",
    "feature_sets = collections.Counter(feature_sets).most_common(len(feature_sets))\n",
    "feature = [f for f,i in feature_sets]\n",
    "f2id = {f:i+1 for i,f in enumerate(feature)}\n",
    "\n",
    "#print f2id['prepunctdigit=True']\n",
    "\n",
    "feature_sets = [(get_feature(tokens,i)) for i in xrange(2, len(tokens)-2)]\n",
    "fid_sets = [[f2id[f] for f in l] for l in feature_sets]\n",
    "#print train_fid_sets[:5]\n",
    "labels = [i in boundaries for i in xrange(2,len(tokens)-2)]\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6697, 38, 417, 1, 4, 2, 3, 6], [397, 6664, 1401, 35, 4, 2, 3, 5], [1358, 410, 14, 1, 4, 2, 3, 5], [13, 1415, 8622, 1, 4, 7, 3, 5], [8238, 15, 164, 1, 8, 2, 3, 5], [162, 8723, 3156, 1, 4, 2, 9, 5], [3299, 163, 20, 1, 4, 2, 3, 6], [19, 3158, 3586, 1, 4, 2, 3, 5], [3620, 21, 12, 1, 4, 2, 3, 5]]\n"
     ]
    }
   ],
   "source": [
    "print(fid_sets[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998 19998\n"
     ]
    }
   ],
   "source": [
    "#filter out too many false example\n",
    "num_to_select = 1  # set the number to select here.\n",
    "def balance_feature():\n",
    "    index_collecter = []\n",
    "    feature = []\n",
    "    label = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == False:\n",
    "            index_collecter.append(i)\n",
    "        if labels[i] == True:\n",
    "            selected_index = random.sample(index_collecter, num_to_select)\n",
    "            for j in selected_index:\n",
    "                feature.append(fid_sets[j])\n",
    "                label.append(labels[j])\n",
    "            feature.append(fid_sets[i])\n",
    "            label.append(labels[i])\n",
    "    return feature,label\n",
    "f,l = balance_feature()\n",
    "print (len(f),len(l))\n",
    "pickle.dump(f,open(\"equal_class_feature\",\"w\"))\n",
    "pickle.dump(l,open(\"equal_class_label\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rename_label(train_labels):\n",
    "    new_labels = []\n",
    "    for i in train_labels:\n",
    "        if i == True:\n",
    "            new_labels.extend([1])\n",
    "        elif i == False:\n",
    "            new_labels.extend([2])\n",
    "    return new_labels\n",
    "y = rename_label(l)\n",
    "pickle.dump(l,open(\"equal_class_num_label\",\"w\"))\n",
    "#print (y[:40])\n",
    "#print (train_labels[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#y = LabelEncoder().fit_transform(train_labels)\n",
    "y_train = labels[:int(len(labels)*0.9)]\n",
    "y_dev = labels[int(len(labels)*0.9):]\n",
    "x_train = fid_sets[:int(len(fid_sets)*0.9)]\n",
    "x_dev = fid_sets[int(len(fid_sets)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(word2id,open(\"word2id\",\"w\"))\n",
    "#pickle.dump(f2id,open(\"f2id\",\"w\"))\n",
    "pickle.dump(x_train,open(\"train_data\",\"w\"))\n",
    "pickle.dump(y_train,open(\"train_label\",\"w\"))\n",
    "pickle.dump(x_dev,open(\"dev_data\",\"w\"))\n",
    "pickle.dump(y_dev,open(\"dev_label\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "x_train = pickle.load(open(\"train_data\"))\n",
    "y_train = pickle.load(open(\"train_label\"))\n",
    "x_dev = pickle.load(open(\"dev_data\"))\n",
    "y_dev = pickle.load(open(\"dev_label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set 190855 190855\n"
     ]
    }
   ],
   "source": [
    "nf = 8\n",
    "nl = 2\n",
    "def reformat(dataset, labels):\n",
    "    dataset = np.array(dataset).reshape((-1, 8)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(2) == np.array(labels)[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "#only once\n",
    "#x_train, y_train = reformat(x_train,y_train)\n",
    "#x_dev, y_dev = reformat(x_dev,y_dev)\n",
    "\n",
    "print('Training set', len(x_train), len(y_train))\n",
    "#print(x_train.dtype,y_train.dtype,x_dev.dtype,y_dev.dtype)\n",
    "#print('Validation set', x_dev.shape, y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions,1) == np.argmax(labels,1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def true_class(p,l):\n",
    "    tp = 0\n",
    "    ta = 0\n",
    "    #p = np.ndarray.tolist(p)\n",
    "    #l = np.ndarray.tolist(l)\n",
    "    for x,y in zip (p,l):\n",
    "        if x == y == True:\n",
    "            tp+=1\n",
    "        if y == True:\n",
    "            ta+=1\n",
    "            \n",
    "    return float(tp/ta)\n",
    "\n",
    "print(true_class(y_dev,y_dev))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "#print (y_train[:35])\n",
    "#print (x_train[:35])\n",
    "print (y_dev[:50])\n",
    "#print (x_dev[:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.929690078856 0.949073419154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "model.fit(x_train, y_train)\n",
    "predict = model.predict(x_dev)\n",
    "print(true_class(predict,y_dev))\n",
    "train_score = model.score(x_train, y_train)\n",
    "test_score = model.score(x_dev, y_dev)\n",
    "print(train_score,test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 0.693350\n",
      "Training accuracy: 4.9%\n",
      "Training true accuracy: 0.0%\n",
      "Validation accuracy: 3.3%\n",
      "Validation true accuracy: 0.0%\n",
      "Minibatch loss at step 100: 0.692180\n",
      "Training accuracy: 95.1%\n",
      "Training true accuracy: 0.0%\n",
      "Validation accuracy: 96.7%\n",
      "Validation true accuracy: 0.0%\n",
      "Minibatch loss at step 200: 0.638872\n",
      "Training accuracy: 95.1%\n",
      "Training true accuracy: 0.0%\n",
      "Validation accuracy: 96.7%\n",
      "Validation true accuracy: 0.0%\n",
      "Minibatch loss at step 300: 0.433518\n",
      "Training accuracy: 95.1%\n",
      "Training true accuracy: 0.0%\n",
      "Validation accuracy: 96.7%\n",
      "Validation true accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "### two layers neural network\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "N, D, H, C = 190855, 8, 8, 2 \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape = [N,D])\n",
    "    y = tf.placeholder(tf.float32, shape = [N,C])\n",
    "    x_d = tf.constant(x_dev)\n",
    "    y_d = tf.constant(y_dev)\n",
    "    dtype = tf.float32\n",
    "    w1 = tf.Variable(1e-4 * np.random.randn(D,H).astype(np.float32))\n",
    "    w2 = tf.Variable(1e-4 * np.random.randn(H,C).astype(np.float32))\n",
    "\n",
    "    def model(data):\n",
    "        #1-2 layer\n",
    "        a = tf.matmul(data,w1) \n",
    "        a_relu = tf.nn.relu(a)\n",
    "\n",
    "        #2-3 layer\n",
    "        scores = tf.matmul(a_relu,w2)\n",
    "        return scores\n",
    "\n",
    "    #loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(model(x), y))\n",
    "\n",
    "    #for tensorboard\n",
    "    loss_summary = tf.scalar_summary(\"loss\",loss)\n",
    "    w1_hist = tf.histogram_summary('w1',w1)\n",
    "    w2_hist = tf.histogram_summary('w2',w2)\n",
    "\n",
    "    #optimize\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    #prediction\n",
    "    p_train = tf.nn.softmax(model(x_train))\n",
    "    p_dev = tf.nn.softmax(model(x_d))\n",
    "\n",
    "    num_steps = 400\n",
    "    batch_size = 1000\n",
    "    with tf.Session() as sess:\n",
    "        merged = tf.merge_all_summaries()\n",
    "        writer = tf.train.SummaryWriter('/home/work/sstmp/ss_log',sess.graph)\n",
    "        #sess.run(tf.initialize_all_tables())\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            feed_dict = {x : x_train, y : y_train}\n",
    "            _, l, predictions = sess.run([optimizer, loss, p_train], feed_dict=feed_dict)\n",
    "            if (step % 100 == 0):\n",
    "                print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                print('Training accuracy: %.1f%%' % accuracy(predictions, y_train))\n",
    "                print('Training true accuracy: %.1f%%' % true_class(predictions, y_train))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(p_dev.eval(),y_dev))\n",
    "                print('Validation true accuracy: %.1f%%' % true_class(p_dev.eval(),y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
