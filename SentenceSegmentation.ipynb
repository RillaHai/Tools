{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentence Segmentation\n",
    "## this script records the experiments of en & zh sentence segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "import nltk.metrics \n",
    "from collections import defaultdict\n",
    "from nltk.classify.util import apply_features, accuracy as eval_accuracy\n",
    "from nltk.metrics import (BigramAssocMeasures, precision as eval_precision,recall as eval_recall, f_measure as eval_f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training corpus and extract features for English sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data time:  3.45624399185\n"
     ]
    }
   ],
   "source": [
    "#training without tokenization \n",
    "t0 = time.time()\n",
    "en = \"/home/work/ssCat/sss.en\"\n",
    "with open(en) as inputen:\n",
    "    tokens = []  #store all the tokes\n",
    "    boundaries = set()   #store boundary index\n",
    "    boundaries_token = set()  #store tokens which is a sentence boundary\n",
    "    offset = 0\n",
    "    for sent in inputen:\n",
    "        #nltk tokenization\n",
    "        #sent = nltk.word_tokenize(sent.decode(\"utf-8\"))\n",
    "        #print sent\n",
    "        sent = sent.strip().split(\" \")\n",
    "        #print sent\n",
    "        if len(sent) >=1:\n",
    "            boundaries_token.add(sent[-1])\n",
    "            tokens.extend(sent)\n",
    "            offset += len(sent)\n",
    "            boundaries.add(offset-1)\n",
    "            \n",
    "\n",
    "\n",
    "def punct_features(tokens, i):\n",
    "    \"\"\"The following features are used for English:\n",
    "    boundary token\n",
    "    last letter of the boundary token\n",
    "    last two letters of the boundary token\n",
    "    previous word of boundary token\n",
    "    next word of boundary token\n",
    "    boolean if previous word is digit\n",
    "    boolean if the first letter of the boundary token is capital\n",
    "    boolean if the first letter of the next word is capital\n",
    "    boolean if the first letter of previous word is capital\n",
    "    boolean if the first letter of the previous 2 word is capital\"\"\"\n",
    "    #print tokens[i]\n",
    "    return {'punct': tokens[i],\\\n",
    "            'punctLastLetter':tokens[i][-1],\\\n",
    "            'punctL2L':tokens[i][-2:],\\\n",
    "            'prepunct':tokens[i-1],\\\n",
    "            'nextpunct':tokens[i+1],\\\n",
    "            'prepunctdigit':tokens[i-1].isdigit(),\\\n",
    "            'punctCapi': tokens[i][0].isupper(),\\\n",
    "            'next1letterCapi': tokens[i+1][0].isupper(),\\\n",
    "            'pre1LetterCap': tokens[i-1][0].isupper(),\\\n",
    "            'pre2LetterCap': tokens[i-2][0].isupper()}\n",
    "\n",
    "#sweeping from begine to end, starting from the third word\n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\\\n",
    "               for i in xrange(2, len(tokens)-2) if tokens[i] in boundaries_token]\n",
    "#split feature data into training developing data and testing data\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "size_dev = int(len(train_set)*0.2)\n",
    "train_set,dev_set = train_set[size_dev:], train_set[:size_dev]\n",
    "print \"processing data time: \",time.time()-t0,\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33755, 23419)\n"
     ]
    }
   ],
   "source": [
    "#vectorize letter feature into i-k \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk import compat\n",
    "def vectorize(labeled_featuresets):\n",
    "    x, y = list(compat.izip(*labeled_featuresets))\n",
    "    v = DictVectorizer()\n",
    "    x =v.fit_transform(x)\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return x,y\n",
    "x,y = vectorize(train_set)\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#two layers neural network\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "N, D, H, C = 64, 10, 5, 2\n",
    "\n",
    "x = tf.placeholder(tf,float32, shape = [None, D])\n",
    "y = tf.placeholder(tf,float32, shape = [None, C])\n",
    "\n",
    "w1 = tf.Variable(1e-3 * np.random.randn(D,H),astype(np.float32))\n",
    "w2 = tf.Variable(1e-3 * np.random.randn(H,C),astype(np.float32))\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "a_relu = tf.nn.relu(a)\n",
    "scores = tf.matmul(a_relu,w2)\n",
    "probs = tf.nn.softmax(scores)\n",
    "loss = -tf.reduce_sum(y*tf.log(probs))\n",
    "\n",
    "#for tensorboard\n",
    "loss_summary = tf.scalar_summary(\"loss\",loss)\n",
    "w1_hist = tf.histogram_summary('w1',w1)\n",
    "w2_hist = tf.histogram_summary('w2',w2)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = np.random.randn(N,D).astype(np.float32)\n",
    "yy = np.zeros((N,C)).astype(np.float32)\n",
    "yy[np.arange(N),np.random.randint((C,size=N))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter('/home/work/sstmp/ss_log',sess.graph_def)\n",
    "    sess.run(tf.initialize_all_tables())\n",
    "    \n",
    "    for t in xrange(100):\n",
    "        summary_str,_,loss_value = sess.run([merged,train_step,loss],\n",
    "                                            feed_dict={x:xx,y:yy})\n",
    "        write.add_summary(summar_str,t)\n",
    "        print \"loss value\".loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Chinese sentence and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634978\n",
      "processing data time:  12.5063760281\n"
     ]
    }
   ],
   "source": [
    "#training with sentence segmentation\n",
    "t0 = time.time()\n",
    "en = \"/home/work/ssCat/sss.zh\"\n",
    "with open(en) as inputen:\n",
    "    tokens = []\n",
    "    boundaries = set()\n",
    "    boundaries_token = set()\n",
    "    offset = 0\n",
    "    for sent in inputen:\n",
    "        sent = sent.strip().split(\"\") #split into characters\n",
    "        if len(sent) >=1:\n",
    "            boundaries_token.add(sent[-1])\n",
    "            tokens.extend(sent)\n",
    "            offset += len(sent)\n",
    "            boundaries.add(offset-1)\n",
    "#print len(tokens)     \n",
    "def punct_features(tokens, i):\n",
    "    \"\"\"Chinese feature:\n",
    "    previous two character\n",
    "    previous word\n",
    "    boolean previous word is digit\n",
    "    boundary word\n",
    "    boolean if boundary word is digit\n",
    "    boolean if next word is digit\n",
    "    \"\"\"\n",
    "    return {'prev-2word': tokens[i-2],\\\n",
    "            'prev-word': tokens[i-1],\\\n",
    "            'predigit': tokens[i-1].isdigit(),\\\n",
    "            'punct': tokens[i],\\\n",
    "            'punctdigit': tokens[i].isdigit(),\\\n",
    "            'next-word-digit':tokens[i+1].isdigit()}\n",
    "\n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\\\n",
    "               for i in range(1, len(tokens)-1) if tokens[i] in boundaries_token]\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "size_dev = int(len(train_set)*0.2)\n",
    "train_set,dev_set = train_set[size_dev:], train_set[:size_dev]\n",
    "print \"processing data time: \",time.time()-t0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(classifier, test_set, accuracy=True, f_measure=True, precision=True, recall=True):\n",
    "        \n",
    "        print(\"Evaluating {} results...\".format(type(classifier).__name__))\n",
    "        metrics_results = {}\n",
    "        if accuracy == True:\n",
    "            accuracy_score = nltk.classify.accuracy(classifier, test_set)\n",
    "            metrics_results['Accuracy'] = accuracy_score\n",
    "\n",
    "        gold_results = defaultdict(set)\n",
    "        test_results = defaultdict(set)\n",
    "        labels = set()\n",
    "        for i, (feats, label) in enumerate(test_set):\n",
    "            labels.add(label)\n",
    "            gold_results[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            test_results[observed].add(i)\n",
    "\n",
    "        for label in labels:\n",
    "            if precision == True:\n",
    "                precision_score = eval_precision(gold_results[label],\n",
    "                    test_results[label])\n",
    "                metrics_results['Precision [{}]'.format(label)] = precision_score\n",
    "            if recall == True:\n",
    "                recall_score = eval_recall(gold_results[label],\n",
    "                    test_results[label])\n",
    "                metrics_results['Recall [{}]'.format(label)] = recall_score\n",
    "            if f_measure == True:\n",
    "                f_measure_score = eval_f_measure(gold_results[label],\n",
    "                    test_results[label])\n",
    "                metrics_results['F-measure [{}]'.format(label)] = f_measure_score\n",
    "\n",
    "        # Print evaluation results (in alphabetical order)\n",
    "        for result in sorted(metrics_results):\n",
    "            print('{}: {}'.format(result, metrics_results[result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  32.6861569881\n",
      "0.952293641459\n",
      "0.955475293458\n",
      "Evaluating SklearnClassifier results...\n",
      "Accuracy: 0.955475293458\n",
      "F-measure [False]: 0.975462438656\n",
      "F-measure [True]: 0.759905977166\n",
      "Precision [False]: 0.96673695667\n",
      "Precision [True]: 0.833517495396\n",
      "Recall [False]: 0.984346862446\n",
      "Recall [True]: 0.698241283554\n"
     ]
    }
   ],
   "source": [
    "#using libsvm\n",
    "t1 = time.time()\n",
    "classifier = SklearnClassifier(LinearSVC()).train(train_set)\n",
    "print \"training time: \",time.time()-t1 \n",
    "print nltk.classify.accuracy(classifier, dev_set)\n",
    "print nltk.classify.accuracy(classifier, test_set)\n",
    "evaluate(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training time:  4158.4049499\n",
      "0.119447145724\n",
      "0.127076350093\n",
      "Evaluating SklearnClassifier results...\n",
      "Accuracy: 0.127076350093\n",
      "F-measure [False]: None\n",
      "F-measure [True]: 0.225497323376\n",
      "Precision [False]: None\n",
      "Precision [True]: 0.127076350093\n",
      "Recall [False]: 0.0\n",
      "Recall [True]: 1.0\n"
     ]
    }
   ],
   "source": [
    "t3 = time.time()\n",
    "classifier = SklearnClassifier(OneClassSVM()).train(train_set)\n",
    "print \"training time: \",time.time()-t3\n",
    "print nltk.classify.accuracy(classifier, dev_set)\n",
    "print nltk.classify.accuracy(classifier, test_set)\n",
    "evaluate(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t4 = time.time()\n",
    "classifier = SklearnClassifier(SVC()).train(train_set)\n",
    "print \"training time: \",time.time()-t4 \n",
    "print nltk.classify.accuracy(classifier, dev_set)\n",
    "print nltk.classify.accuracy(classifier, test_set)\n",
    "evaluate(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  1.87185192108\n",
      "0.967328177585\n",
      "0.969388849178\n",
      "Most Informative Features\n",
      "                   punct = '?'              True : False  =    118.0 : 1.0\n",
      "                   punct = ','             False : True   =     87.6 : 1.0\n",
      "                   punct = 'f'             False : True   =     86.5 : 1.0\n",
      "                   punct = '.'              True : False  =     69.8 : 1.0\n",
      "                   punct = '}'              True : False  =     48.5 : 1.0\n",
      "                   punct = 'o'             False : True   =     46.7 : 1.0\n",
      "                   punct = '!'              True : False  =     45.2 : 1.0\n",
      "                   punct = 'u'             False : True   =     39.1 : 1.0\n",
      "                   punct = '\\xa6'           True : False  =     38.0 : 1.0\n",
      "                   punct = '-'             False : True   =     31.0 : 1.0\n",
      "None\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.969388849178\n",
      "F-measure [False]: 0.983870815875\n",
      "F-measure [True]: 0.700262467192\n",
      "Precision [False]: 0.976013450402\n",
      "Precision [True]: 0.823456790123\n",
      "Recall [False]: 0.991855719032\n",
      "Recall [True]: 0.609132420091\n"
     ]
    }
   ],
   "source": [
    "#using naive bayes\n",
    "t2 = time.time()\n",
    "classifier = classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print \"training time: \",time.time()-t2\n",
    "print nltk.classify.accuracy(classifier, dev_set)\n",
    "print nltk.classify.accuracy(classifier, test_set)\n",
    "print classifier.show_most_informative_features()\n",
    "evaluate(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!;' and i != len(words)-1 and classifier.classify(punct_features(words, i)) == True:             #\n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents\n",
    "\n",
    "teststring = \"If you have your own collection of text files that you would\\\n",
    " like to access using the above methods, you can easily load them with\\\n",
    "  the help of NLTK's PlaintextCorpusReader. Check the location of your \\\n",
    "  files on your file system; in the following example, we have taken \\\n",
    "  this to be the directory /usr/share/dict. Whatever the location, \\\n",
    "  set this to be the value of corpus_root.\"\n",
    "\n",
    "wl = nltk.word_tokenize(teststring)\n",
    "ss = segment_sentences(wl)\n",
    "print len(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *select even number sentence with sed*\n",
    "\n",
    "`sed -n 2~2p rall.en > first`\n",
    "`sed -n 'n;p'`\n",
    "\n",
    "* *select odd number sentence with sed*\n",
    "\n",
    "`sed -n 1~2p first >second`\n",
    "`sed -n 'p;n'`\n",
    "\n",
    "* *select tandomly about 1% of the total corpus*\n",
    "\n",
    "`perl -ne 'print if (rand() < .01)' forth > fifth`\n",
    "\n",
    "* *select sentence with length shorten than 60 tokens*\n",
    "\n",
    "`awk 'length($0)<60' fifth >sixth`\n",
    "\n",
    "* *select sentence exceed 80*\n",
    "\n",
    "`grep '^.\\{80\\}' file`\n",
    "\n",
    "`perl -nle 'print if length$_>79' file`\n",
    "\n",
    "`sed -n '/.\\{80\\}/p' file`       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一双灵巧的手飞舞着操纵着键盘和鼠标，富有节奏的敲击声仿佛是一首轻快的乐章。屏幕上漫天的光华闪过，对手飞扬着血花倒了下去。\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe3 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e18ea940f7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#splitText(readText(f))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtmpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mu\"一双灵巧的手飞舞着操纵着键盘和鼠标，富有节奏的敲击声仿佛是一首轻快的乐章。屏幕上漫天的光华闪过，对手飞扬着血花倒了下去。\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mu'“呵呵。”叶秋笑了笑，抬手取下了衔在嘴角的烟头。银白的烟灰已经结成了长长一串，但在叶秋挥舞着鼠标敲打着键盘施展操作的过程中却没有被震落分毫。摘下的烟头很快被掐灭在了桌上的一个形状古怪的烟'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msplitText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e18ea940f7f2>\u001b[0m in \u001b[0;36msplitText\u001b[0;34m(senlist)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(\"。”\")|(\"？”\")|(\"！”\")|(\"？！\")|(\"！？\")|(\"！！！\")|(\"……\")|(\"？？？\")|。|,|(\"？\")|(\"！\")|(\";\")'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"。\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#f = codecs.open(\"/home/bear/experiments/30429.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe3 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "#! /bin/python\n",
    "#-*- coding:utf-8 -*-\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def readText(text):\n",
    "    sentences = []\n",
    "    for line in text:\n",
    "\tline = line.strip()\n",
    "\tline = sub(line)\n",
    "\tif line != '':\n",
    "\t    sentences.append(line)\n",
    "    return sentences\n",
    "\t\n",
    "def sub(text):\n",
    "    rep = {\"/n+\": \"\", \"/s+\": \"\",\"/t+\":\"\"} # define desired replacements here\n",
    "    # use these three lines to do the replacement\n",
    "    rep = dict((re.escape(k), v) for k, v in rep.iteritems())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
    "    return text\n",
    "\t\n",
    "def splitText(senlist):\n",
    "    for i in senlist:\n",
    "        split = re.split('(\"。”\")|(\"？”\")|(\"！”\")|(\"？！\")|(\"！？\")|(\"！！！\")|(\"……\")|(\"？？？\")|。|,|(\"？\")|(\"！\")|(\";\")',i)\n",
    "        print i\n",
    "        split = i.split(\"。\")\n",
    "        print \"\".join(split)\n",
    "#f = codecs.open(\"/home/bear/experiments/30429.txt\")\n",
    "#splitText(readText(f))\n",
    "tmpl = [u\"一双灵巧的手飞舞着操纵着键盘和鼠标，富有节奏的敲击声仿佛是一首轻快的乐章。屏幕上漫天的光华闪过，对手飞扬着血花倒了下去。\",u'“呵呵。”叶秋笑了笑，抬手取下了衔在嘴角的烟头。银白的烟灰已经结成了长长一串，但在叶秋挥舞着鼠标敲打着键盘施展操作的过程中却没有被震落分毫。摘下的烟头很快被掐灭在了桌上的一个形状古怪的烟']\n",
    "splitText(tmpl)\n",
    "\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
